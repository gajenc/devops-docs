---
description: Docker-in-Docker
---

# 9. Kaniko

Interestingly, the Docker Desktop isn’t the only thing that can create Docker images. [Kaniko](https://github.com/GoogleContainerTools/kaniko) is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster. It executes each command from a Dockerfile in userspace and doesn’t communicate with the Docker daemon. It’s from a small company that made a search engine a bit like Altavista. 

{% embed url="https://youtu.be/-OpZEIIIyUw" %}



### Jenkins Pipeline <a href="f699" id="f699"></a>

In the DevOps course, we have a Jenkins pipeline that runs through a bunch of tasks sequentially. The first is to build a container image, and it’s this stage that currently uses Docker-in-Docker.

* **Build:** Use Docker-in-Docker to build an image of a simple Flask Web App that’s developed in the course — move to Kaniko!
* **Test:** Lint (stating analysis), functional tests, and “Behaviour Driven Development” tests.
* **SonarQube:** More static analysis.
* **Documentation:** Generate documentation from the code.
* **Deploy:** Deploy the developed app to our environment.
* **Performance Test:** Use Gatling to “hammer” the application and measure its response under load.

Anyway, I digress, it’s the **Build** step we’re talking about in this article.

### Jenkinsfile Build Step <a href="e784" id="e784"></a>

It uses a trigger that runs once a minute to check for changes in the Git repository. The \`Build\` stage is populated, but the other stages are waiting to be completed later in the course. Let’s take a look at the \`Build\` stage:

```
pipeline {
  agent none//check every minute for changes
  triggers {
    pollSCM('*/1 * * * *')
  }  stages {
    //Build container image
    stage('Build') {
      agent {
        kubernetes {
          label 'jenkinsrun'
          defaultContainer 'dind'
          yaml """
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: dind
    image: docker:18.05-dind
    securityContext:
      privileged: true
    volumeMounts:
      - name: dind-storage
        mountPath: /var/lib/docker
  volumes:
    - name: dind-storage
      emptyDir: {}
"""
        }
      }
      steps {
        container('dind') {
          script {
            //we need some extra packages installing
            sh 'apk -Uuv add make groff less python py-pip'//we need the AWS command line tools
            sh 'pip install awscli'//we need to authenticate with AWS
            sh '$(aws ecr get-login --no-include-email --region eu-west-1)'//tell jenkins where to save the image
            //notice this is https:// followed by the repositoryUri you created earlier
            docker.withRegistry('https://AWSACCOUNT.dkr.ecr.eu-west-1.amazonaws.com/app') {//build the image
              def customImage = docker.build("app:${env.BUILD_ID}")//upload it to the registry
              customImage.push()
            }
          } //script
        } //container
      } //steps
    } //stage(build)    //Test goes here    //SonarQube goes here    //Documentation generation goes here    //Deploy goes here    //Performance testing goes here
  } //stages
} //pipeline
```

It uses a Kubernetes agent to spin-up a Pod from the docker:18.05-dind image. It also installs the tooling we need to authenticate and push the built image to Amazon’s Elastic Container Registry. This tooling is downloaded and installed every time we run the pipeline, which is far from ideal. What happens if we try to swap to Kaniko?

### Kaniko <a href="10a5" id="10a5"></a>

Kaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster. Kaniko doesn’t depend on a Docker daemon and executes each command within a Dockerfile completely in userspace. We’ve heard about the benefits of Kaniko, but how do we change the \`Jenkinsfile\` above to use it? Is it easy or hard?

We’re running on AWS, so we need to tell Kaniko how to authenticate with Amazon’s Elastic Container Registry. That’s easy enough to do by adding a new Config-Map to our cluster. We create a YAML file called \`kaniko.yml\` populated with:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: docker-config
data:
  config.json: |-
    {
       "credsStore": "ecr-login"
    }
```

and apply it to our Kubernetes cluster with \`kubectl apply -f kaniko.yml\`. That replaces all the tooling lines we used to need for the AWS CLI, since Kankio knows how to authenticate with ECR already. Then we just need to replace the Build step in our \`Jenkinsfile\` with:

```
stage('Build') {
  agent {
    kubernetes {
      label 'jenkinsrun'
      defaultContainer 'builder'
      yaml """
kind: Pod
metadata:
  name: kaniko
spec:
  containers:
  - name: builder
    image: gcr.io/kaniko-project/executor:debug
    imagePullPolicy: Always
    command:
    - /busybox/cat
    tty: true
    volumeMounts:
      - name: docker-config
        mountPath: /kaniko/.docker
  volumes:
    - name: docker-config
      configMap:
        name: docker-config
"""
    }
  }steps {
      script {
        sh "/kaniko/executor --dockerfile `pwd`/Dockerfile --context `pwd` --destination=AWSACCOUNT.dkr.ecr.eu-west-1.amazonaws.com/app:${env.BUILD_ID}"
    } //container
  } //steps
} //stage(build)
```

And we’re done! It mounts the ConfigMap and points to the Kaniko project’s executor. Pretty seamless to be honest, and much neater!

###  <a href="0ed7" id="0ed7"></a>
